{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Adversarial Autoencoder (Basic Implementation)",
   "id": "cc6f0c4da6afad06"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-29T10:33:17.398501Z",
     "start_time": "2025-04-29T10:33:17.396130Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from components import AdversarialAutoencoder"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset Configuration",
   "id": "d6e7501bcda3f6b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T10:26:19.695450Z",
     "start_time": "2025-04-29T10:26:19.692702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def configure_mnist(batch_size=100):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(), # [0,1 normalization]\n",
    "        transforms.Lambda(lambda x : x.view(-1)) # [(n, d, d, 1) -> (n,d^2)]\n",
    "    ])\n",
    "\n",
    "    return DataLoader(\n",
    "        datasets.MNIST(root='./data', train=True, transform=transform, download=True),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n"
   ],
   "id": "1d205ac29e836e51",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paper Configuration",
   "id": "6ac1fa5ebb04e182"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T10:37:33.982531Z",
     "start_time": "2025-04-29T10:37:33.980144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "INPUT_DIM = 784\n",
    "BATCH_SIZE = 100\n",
    "AE_HIDDEN = 1000\n",
    "DC_HIDDEN = 1000\n",
    "LATENT_DIM = 8\n",
    "PRIOR_STD = 5.0\n",
    "recon_loss = nn.MSELoss()\n",
    "learning_rate = 1e-3 # not specified in appendix A.1, for successive experiments it is\n",
    "use_decoder_sigmoid = True\n"
   ],
   "id": "1b0bb15c7c03c19d",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "beaffbcac085f1c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T10:43:28.080933Z",
     "start_time": "2025-04-29T10:37:59.975787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "aae = AdversarialAutoencoder(\n",
    "    input_dim=INPUT_DIM,\n",
    "    ae_hidden=AE_HIDDEN,\n",
    "    dc_hidden=DC_HIDDEN,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    recon_loss_fn=recon_loss,\n",
    "    lr=learning_rate,\n",
    "    use_decoder_sigmoid=use_decoder_sigmoid,\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "loader = configure_mnist(batch_size=BATCH_SIZE)\n",
    "\n",
    "aae.train(\n",
    "    data_loader=loader,\n",
    "    epochs=50,\n",
    "    prior_std=PRIOR_STD,\n",
    ")"
   ],
   "id": "38ae34fd974a4959",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (1/50)\t)Recon Loss: 0.0787\t)Disc Loss: 1.4262\t)Gen Loss: 1.6439\t)\n",
      "Epoch (2/50)\t)Recon Loss: 0.0704\t)Disc Loss: 1.8373\t)Gen Loss: 2.6186\t)\n",
      "Epoch (3/50)\t)Recon Loss: 0.0673\t)Disc Loss: 1.1156\t)Gen Loss: 2.3606\t)\n",
      "Epoch (4/50)\t)Recon Loss: 0.0671\t)Disc Loss: 0.6345\t)Gen Loss: 4.8821\t)\n",
      "Epoch (5/50)\t)Recon Loss: 0.0672\t)Disc Loss: 0.6537\t)Gen Loss: 4.4879\t)\n",
      "Epoch (6/50)\t)Recon Loss: 0.0673\t)Disc Loss: 0.2102\t)Gen Loss: 4.5482\t)\n",
      "Epoch (7/50)\t)Recon Loss: 0.0662\t)Disc Loss: 0.1323\t)Gen Loss: 4.8886\t)\n",
      "Epoch (8/50)\t)Recon Loss: 0.0655\t)Disc Loss: 0.1692\t)Gen Loss: 5.1607\t)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 14\u001B[39m\n\u001B[32m      1\u001B[39m aae = AdversarialAutoencoder(\n\u001B[32m      2\u001B[39m     input_dim=INPUT_DIM,\n\u001B[32m      3\u001B[39m     ae_hidden=AE_HIDDEN,\n\u001B[32m   (...)\u001B[39m\u001B[32m      9\u001B[39m     device = \u001B[33m\"\u001B[39m\u001B[33mcuda\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch.cuda.is_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     10\u001B[39m )\n\u001B[32m     12\u001B[39m loader = configure_mnist(batch_size=BATCH_SIZE)\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m \u001B[43maae\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m50\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m    \u001B[49m\u001B[43mprior_std\u001B[49m\u001B[43m=\u001B[49m\u001B[43mPRIOR_STD\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/school/adversarial-autoencoders/src/adversarial_autoencoder/components.py:106\u001B[39m, in \u001B[36mAdversarialAutoencoder.train\u001B[39m\u001B[34m(self, data_loader, epochs, prior_std)\u001B[39m\n\u001B[32m    103\u001B[39m x_hat = \u001B[38;5;28mself\u001B[39m.decoder(z)\n\u001B[32m    105\u001B[39m recon_loss = \u001B[38;5;28mself\u001B[39m.recon_loss(x_hat, x)\n\u001B[32m--> \u001B[39m\u001B[32m106\u001B[39m \u001B[43mrecon_loss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    107\u001B[39m \u001B[38;5;28mself\u001B[39m.ae_opt.step()\n\u001B[32m    109\u001B[39m \u001B[38;5;66;03m# part 1: backprop through discriminator\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.cache/pypoetry/virtualenvs/adversarial-autoencoder-O6AyNR-z-py3.12/lib/python3.12/site-packages/torch/_tensor.py:626\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    616\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    617\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    618\u001B[39m         Tensor.backward,\n\u001B[32m    619\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    624\u001B[39m         inputs=inputs,\n\u001B[32m    625\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m626\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    628\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.cache/pypoetry/virtualenvs/adversarial-autoencoder-O6AyNR-z-py3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    342\u001B[39m     retain_graph = create_graph\n\u001B[32m    344\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    345\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    346\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m347\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    348\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    349\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    350\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    351\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    352\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    353\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.cache/pypoetry/virtualenvs/adversarial-autoencoder-O6AyNR-z-py3.12/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    821\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    822\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m823\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    824\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    825\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    826\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    827\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3c342f55b0afaf44"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
